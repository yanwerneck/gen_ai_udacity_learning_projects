# Simple RAG Chatbot

A project demonstrating retrieval-augmented generation (RAG) and conversational AI with memory management using OpenAI embeddings and the Completion API.

## Project Overview

This project explores core concepts of building RAG-powered chatbots:

1. **Embeddings & Semantic Search** - Using OpenAI embeddings to find semantically similar documents
2. **Retrieval-Augmented Generation** - Augmenting LLM prompts with relevant context from a knowledge base
3. **Token Management** - Carefully managing token budgets within LLM attention windows

## Project Structure

```
simple_rag_chatbot/
├── README.md
├── rag_data/
│   ├── character_descriptions.csv
│   └── character_descriptions_embeddings.csv
├── notebooks/
│   └── simple_chatbot.ipynb
├── requirements.txt
└── activate.sh
```

## Notebooks

### Custom Chatbot with RAG

**File**: `notebooks/simple_chatbot.ipynb`

This notebook implements a RAG system that answers questions about fictional characters using embeddings and semantic search.

#### Dataset

**Source**: `rag_data/character_descriptions.csv`

The dataset contains descriptions of 55 fictional characters from theater, television, and film productions. All characters were AI-generated by OpenAI.

**Structure**:
- `Name` - Character name
- `Description` - Character background and traits
- `Medium` - Type of production (Play, TV, Film)
- `Setting` - Geographic/time setting (e.g., England, Victorian Era)
- `text` - Combined formatted text used for embeddings and retrieval

Example rows:
```
Name: Emily, Description: A young woman in her early 20s, aspiring actress...
Name: Jack, Description: A middle-aged man in his 40s, successful businessman...
Name: Alice, Description: A woman in her late 30s, warm and nurturing...
```

#### Implementation Architecture

**1. Data Preparation**

```python
def get_context_text(row):
    # Formats each row into searchable text
    context_list = [
        f"name: {row['Name']}",
        f"description: {row['Description']}",
        f"medium: {row['Medium']}",
        f"setting: {row['Setting']}"
    ]
    return " - ".join(context_list)
```

**2. Embedding Generation**

- **Model**: `text-embedding-3-large`
- **Process**: Batch embeddings in groups of 20 for efficiency
- **Output**: Stored as `character_descriptions_embeddings.csv`

```python
embeddings = []
for i in range(0, total_batch_size, batch_size):
    batch_text_list = df_context.iloc[i:i+batch_size]["text"].tolist()
    emb_response = openai.Embedding.create(
        input=batch_text_list,
        engine=EMBEDDING_MODEL_NAME
    )
    embeddings.extend([data["embedding"] for data in emb_response["data"]])
```

**3. Semantic Search**

```python
def get_most_similars(query, data_frame, engine=EMBEDDING_MODEL_NAME):
    # Generate embedding for user query
    query_embedding = get_embedding(query, engine=engine)

    # Calculate cosine distance to all character embeddings
    query_distances = distances_from_embeddings(
        query_embedding,
        data_frame["embeddings"].values,
        distance_metric="cosine"
    )

    # Sort by relevance (closest first)
    df_query["distances"] = query_distances
    df_query.sort_values(["distances"], ascending=True)

    return df_query
```

**4. Context-Aware Prompt Building**

```python
def create_prompt(question, df, max_token_count):
    # Build prompt template
    prompt_template = """Answer the question based on the context below,
    and if the question can't be answered based on the context, say "I don't know"

    Context:
    {}
    ---
    Question: {}
    Answer:"""

    # Add most relevant characters up to token limit
    context = []
    for ordered_text in get_most_similars(question, df)["text"].values:
        # Token count check using tiktoken
        if (token_count(ordered_text) + token_count(prompt)) <= max_token_count:
            context.append(ordered_text)

    return prompt_template.format("\n\n###\n\n".join(context), question)
```

**5. LLM Completion**

```python
def get_chat_answer(question, dataframe=df_context,
                    max_prompt_tokens=3800,
                    max_answer_tokens=200,
                    llm_name=LLM_NAME):

    prompt = create_prompt(question, dataframe, max_prompt_tokens)
    answer = openai.Completion.create(
        model=llm_name,
        prompt=prompt,
        max_tokens=max_answer_tokens
    )
    return answer["choices"][0]["text"].strip()
```

#### Performance Demonstration

**Question 1**: "Who is Emily?"

- **Without RAG (Basic)**:
  ```
  "I'm not sure! Could you provide some context? Thanks!"
  ```

- **With RAG**:
  ```
  "Emily is an aspiring actress and Alice's daughter. She is also in a relationship
  with George and is in her early 20s."
  ```

**Question 2**: "Who is well-traveled and cultured, with a beautiful voice?"

- **Without RAG (Basic)**:
  ```
  "Michael Palin"
  ```

- **With RAG**:
  ```
  "Prince Lorenzo"
  ```

**Key Insight**: RAG provides domain-specific, accurate answers by retrieving relevant character data, while basic completion hallucinates or provides generic responses.

#### Configuration

```python
EMBEDDING_MODEL_NAME = "text-embedding-3-large"
LLM_NAME = "gpt-3.5-turbo-instruct"

# Token budgets
MAX_PROMPT_TOKENS = 3800        # Context + question
MAX_ANSWER_TOKENS = 200         # Completion response
BATCH_SIZE = 20                 # Embeddings per API call
```

---

## Key Technical Concepts

### 1. Embeddings

Embeddings convert text into high-dimensional vectors that capture semantic meaning:
- **Model**: `text-embedding-3-large` - 3072 dimensions
- **Purpose**: Enable semantic similarity comparison
- **Distance Metric**: Cosine distance (0 = identical, 1 = opposite)

### 2. RAG Architecture

Retrieval-Augmented Generation improves LLM responses by:
1. Embedding the user query
2. Finding semantically similar documents
3. Including relevant documents in the prompt
4. Generating answer grounded in the retrieved context

### 3. Token Management

Using `tiktoken` encoder (`cl100k_base`) to:
- Count tokens in embeddings and prompt
- Respect the 4,097 token limit for gpt-3.5-turbo-instruct
- Greedily add relevant context within token budget

### 4. Batch Efficiency

Embeddings are generated in batches of 20:
- Reduces API overhead
- Stays within rate limits
- Results saved to CSV checkpoint for later reuse

---

## Requirements

```
pandas
openai
tiktoken
numpy
```

## Installation

```bash
pip install -r requirements.txt
```

Set your OpenAI API key:
```python
openai.api_key = "your-api-key"
openai.api_base = "https://api.openai.com/v1"  # or your custom endpoint
```

## Usage

### Run the Chatbot Notebook
```bash
jupyter notebook notebooks/simple_chatbot.ipynb
```

### Generate Embeddings (if needed)
The notebook includes code to generate embeddings from scratch:
```python
# Batch embed all characters
embeddings = []
for i in range(0, len(df_context), batch_size):
    response = openai.Embedding.create(
        input=df_context.iloc[i:i+batch_size]["text"].tolist(),
        engine=EMBEDDING_MODEL_NAME
    )
    embeddings.extend([d["embedding"] for d in response["data"]])

df_context["embeddings"] = embeddings
df_context.to_csv("rag_data/character_descriptions_embeddings.csv")
```

### Query the RAG System
```python
# Ask a question about characters
answer = get_chat_answer("Who is Emily?")
print(answer)
```

## Environment Setup

Activate the virtual environment:
```bash
source activate.sh  # On macOS/Linux
```

## Data Flow

```
character_descriptions.csv
         ↓
   [Formatting]
         ↓
   [Embedding Generation]
         ↓
character_descriptions_embeddings.csv
         ↓
   [User Query] → [Semantic Search] → [Context Selection] → [Prompt Building] → [LLM] → [Answer]
```

## Key Learnings

1. **RAG Improves Accuracy**: Domain-specific context makes LLM responses factually grounded
2. **Embeddings Enable Search**: Semantic search finds relevant content better than keyword matching
3. **Token Budgeting is Critical**: Must balance context size with completion length
4. **Batch Processing Scales**: Embedding generation in batches is more efficient
5. **Checkpoint Data**: Save embeddings to avoid regenerating them

## Future Enhancements

- Add more character relationships in the CSV (parent, friend, rival)
- Implement multi-turn conversations with memory
- Use stronger embedding models (GPT-4 embeddings when available)
- Add reranking to filter top-k results before including
- Build a vector database (Pinecone, Weaviate) for large-scale deployments
- Implement conversation history with RAG for context carryover
